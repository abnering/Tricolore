"""
Created on 01 13, 2024

evaluate model
@author: abnerzxzhao
"""

import tensorflow as tf
import numpy as np
import math
from tensorflow.python.keras.initializers import truncated_normal, random_normal, glorot_normal, zeros
from tensorflow.keras.layers import Layer, Dense
from tensorflow.keras.losses import Loss

#from deepctr.layers.utils import reduce_max, reduce_mean, reduce_sum, concat_func, div, softmax

class DIS_Layer(Layer):
    def __init__(self):
        super(DIS_Layer,self).__init__()
    def call(self, inputs, **kwargs):
        user_emb, item_emb = inputs
        final_layer = tf.nn.relu((user_emb - item_emb)*user_emb - item_emb)
        print (final_layer)
        return final_layer

def LabelAwareAttention(user_embed, target_emb, pow_p=3, k_max=1):
    target_emb = tf.reshape(target_emb,[-1,1,tf.shape(target_emb)[1]])
    weight = tf.reduce_sum(user_embed * target_emb, axis=-1, keepdims=True)
    #weight = tf.pow(weight, pow_p)  # [x,k_max,1]
    weight = tf.nn.softmax(weight, axis=1, name="weight")
    #weight = tf.pow(weight, pow_p)*5000
    #weight = 5*weight
    output = tf.reduce_sum(user_embed * weight, axis=1)
    return output

def squash(inputs):
    vec_squared_norm = tf.reduce_sum(tf.square(inputs), axis=-1, keepdims=True)
    scalar_factor = vec_squared_norm / (1 + vec_squared_norm) / tf.sqrt(vec_squared_norm + 1e-8)
    vec_squashed = scalar_factor * inputs
    return vec_squashed

class ROUTING(Layer):
    def __init__(self,num_outputs,max_len,num_dims):
        super(ROUTING,self).__init__()
        self.num_outputs = num_outputs
        self.max_len = max_len
        self.num_dims = num_dims
        self.routing_logits = tf.Variable(initial_value=tf.random.uniform(shape=(1, num_outputs, max_len)),name="routing/routing_logits",shape=(1, num_outputs, max_len),dtype=tf.float32,trainable = False)
        self.bilinear_mapping_matrix = tf.Variable(initial_value=tf.random.uniform(shape=(num_dims, num_dims)),name="routing/bilinear_mapping_matrix",shape=(num_dims, num_dims),dtype=tf.float32)
    def call(self, inputs, **kwargs):
        k_max = self.num_outputs
        action_embedding,seq_len = inputs
        iteration_times = 4
        batch_size = tf.shape(action_embedding)[0]
        seq_len_tile = tf.tile(seq_len, [1, k_max])
        for i in range(iteration_times):
            mask = tf.sequence_mask(seq_len_tile,self.max_len)
            pad = tf.ones_like(mask, dtype=tf.float32) * (-2 ** 32 + 1)
            routing_logits_with_padding = tf.where(mask, tf.tile(self.routing_logits, [batch_size, 1, 1]), pad)
            weight = tf.nn.softmax(routing_logits_with_padding)
            behavior_embdding_mapping = tf.tensordot(action_embedding, self.bilinear_mapping_matrix, axes=1)
            Z = tf.matmul(weight, behavior_embdding_mapping)
            interest_capsules = squash(Z)
            delta_routing_logits = tf.reduce_sum(tf.matmul(interest_capsules, tf.transpose(behavior_embdding_mapping, perm=[0, 2, 1])),axis=0, keepdims=True)
            self.routing_logits.assign_add(delta_routing_logits)
            interest_capsules = tf.reshape(interest_capsules, [-1, k_max, self.num_dims])
        return interest_capsules,weight

    



class DOT_Layer(Layer):
    def __init__(self):
        super(DOT_Layer,self).__init__()
    def call(self, inputs, **kwargs):
        user_emb, item_emb = inputs
        final_layer = tf.multiply(user_emb, item_emb)
        print (final_layer)
        return tf.reduce_sum(final_layer,axis=1)



class SelfAttention_Layer(Layer):
    def __init__(self):
        super(SelfAttention_Layer, self).__init__()

    def build(self, input_shape):
        self.dim = input_shape[0][-1]
        self.W = self.add_weight(shape=[self.dim, self.dim], name='weight', 
            initializer='random_uniform')

    def call(self, inputs, **kwargs):
        q, k, v, mask = inputs
        # pos encoding
        k += self.positional_encoding(k)
        q += self.positional_encoding(q)
        # Nonlinear transformation
        q = tf.nn.relu(tf.matmul(q, self.W))  # (None, seq_len, dim)
        k = tf.nn.relu(tf.matmul(k, self.W))  # (None, seq_len, dim)
        mat_qk = tf.matmul(q, k, transpose_b=True)  # (None, seq_len, seq_len)
        dk = tf.cast(self.dim, dtype=tf.float32)
        # Scaled
        scaled_att_logits = mat_qk / tf.sqrt(dk)
        # Mask
        mask = tf.tile(tf.expand_dims(mask, 1), [1, q.shape[1], 1])  # (None, seq_len, seq_len)
        paddings = tf.ones_like(scaled_att_logits) * (-2 ** 32 + 1)
        outputs = tf.where(tf.equal(mask, 0), paddings, scaled_att_logits)  # (None, seq_len, seq_len)
        # softmax
        outputs = tf.nn.softmax(logits=outputs, axis=-1)  # (None, seq_len, seq_len)
        # output
        outputs = tf.matmul(outputs, v)  # (None, seq_len, dim)
        outputs = tf.reduce_mean(outputs, axis=1)  # (None, dim)
        return outputs

    @staticmethod
    def get_angles(pos, i, d_model):
        angle_rates = 1 / np.power(10000, (2 * (i // 2)) / np.float32(d_model))
        return pos * angle_rates

    def positional_encoding(self, QK_input):
        angle_rads = self.get_angles(np.arange(QK_input.shape[1])[:, np.newaxis],
                                np.arange(self.dim)[np.newaxis, :], self.dim)
        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])
        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])
        pos_encoding = angle_rads[np.newaxis, ...]

        return tf.cast(pos_encoding, dtype=tf.float32)

class Composition_Layer(Layer):
    def __init__(self):
        super(Composition_Layer, self).__init__()
    def call(self, inputs, **kwargs):
        user_emb, item_emb,selected_memory = inputs
        energy = (item_emb - user_emb) + selected_memory
        final_layer = tf.sqrt(tf.reduce_sum(tf.square(energy), 1) + 1E-3)
        final_layer = tf.reshape(final_layer,[-1,1])
        return final_layer


class CompositionBPR_Layer(Layer):
    def __init__(self):
        super(CompositionBPR_Layer, self).__init__()
    def call(self, inputs, **kwargs):
        user_emb, item_emb = inputs
        energy = tf.multiply(user_emb,item_emb) 
        final_layer = tf.sqrt(tf.reduce_sum(tf.square(energy), 1) + 1E-3)
        final_layer = tf.reshape(final_layer,[-1,1])
        return -final_layer

class CompositionCML_Layer(Layer):
    def __init__(self):
        super(CompositionCML_Layer, self).__init__()
    def call(self, inputs, **kwargs):
        user_emb, item_emb = inputs
        energy = (item_emb - user_emb)
        final_layer = tf.sqrt(tf.reduce_sum(tf.square(energy), 1) + 1E-3)
        final_layer = tf.reshape(final_layer,[-1,1])
        return -final_layer



class DNN_L(Layer):
    def __init__(self,hidden="64:64",input_dim=64,name="user"):
        super(DNN_L,self).__init__()
        NET = []
        i = 0
        hh = [int(v) for v in hidden.split(":")]

        while i < len(hh):
            NET.append(Dense(hh[i]))
            print (NET[i])
            i += 1
        self.NET = NET
        self.LL = len(self.NET)
    def call(self, inputs, **kwargs):
        for j in range(self.LL):
            inputs = self.NET[j](inputs)
        return inputs

